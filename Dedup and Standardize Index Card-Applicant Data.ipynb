{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import funcy\n",
    "import re\n",
    "import os\n",
    "import uuid\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_cleaning_functions import (trans_remov_punc, standardize_whitespace, remove_punc, remove_suffix_from_last_name,\n",
    "                                     clean_names, has_award, has_suffix, get_suffix, replace_last_name, \n",
    "                                     is_year_range, str_sim, clean_med_school, clean_std_college_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yellow_beret/Data/applicant_data/raw_card_data\n"
     ]
    }
   ],
   "source": [
    "CARD_DATA_DIR = os.path.abspath('Data/applicant_data/raw_card_data')\n",
    "APP_DATA_DIR = os.path.abspath('Data/applicant_data')\n",
    "ATT_DATA_DIR = os.path.abspath('Data/attendees_data')\n",
    "print CARD_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_CSV = False\n",
    "RAW_APPLICANT_DATA_FILENAME = 'raw_applicant_card_data.csv'\n",
    "MISSING_APPDATE_FILENAME = 'index_cards_no_application_date.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLEAN_NAMES = ['clean_first_name', 'clean_middle_name', 'clean_last_name']\n",
    "NAMES_COLS = ['first_name', 'middle_name', 'last_name']\n",
    "PERSONAL_INFO = [\n",
    "    'clean_first_name', 'clean_last_name', 'clean_middle_name',\n",
    "    'date_of_birth', 'medical_school', 'clean_college_trans']\n",
    "\n",
    "AWARDS_KEYWORDS = ['HONORS', 'AWARD', 'HONOR', 'SOCIETY', 'SCHOLAR', 'AOA', 'PME', 'FNHS', 'ODK']\n",
    "\n",
    "# id column that links back to raw applicant data file\n",
    "RAW_CARD_ID = 'raw_uuid'\n",
    "\n",
    "# column where the raw id information is stored\n",
    "RAW_INDEX_IDS = 'raw_card_ids'\n",
    "\n",
    "# try to get one id per unique applicant in the dataset\n",
    "PERSON_ID = 'person_uuid'\n",
    "# id per deduped application-person - if someone applied multiple times, they will have multiple ids\n",
    "PERSON_APPLICATION_ID = 'person_app_uuid' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_appcards2 = pd.read_csv(os.path.join(CARD_DATA_DIR, RAW_APPLICANT_DATA_FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:32.664982",
     "start_time": "2016-09-01T20:19:32.628653"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop all rows with first, last name NA\n",
    "all_app3 = all_appcards2.dropna(subset=['application_date'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/local/lib/python2.7/site-packages/pandas/core/indexing.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/local/lib/python2.7/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "all_app3.loc[pd.isnull(all_app3.application_date), 'flag_missing_app_date'] = 1\n",
    "all_app3.loc[~pd.isnull(all_app3.application_date), 'flag_missing_app_date'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_app_date = all_appcards2.loc[pd.isnull(all_appcards2.application_date), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if OUTPUT_CSV:\n",
    "    missing_app_date.to_csv(os.path.join(CARD_DATA_DIR, MISSING_APPDATE_FILENAME), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:33.174821",
     "start_time": "2016-09-01T20:19:33.156078"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_poorlyfmtdates(str_date):\n",
    "    try:\n",
    "        dt = pd.to_datetime(str_date, format='%m/%d/%Y')\n",
    "        return True\n",
    "    except (ValueError, AssertionError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:34.881145",
     "start_time": "2016-09-01T20:19:33.634962"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = all_app3.application_date.apply(id_poorlyfmtdates)\n",
    "# all_app3.loc[~mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:35.363664",
     "start_time": "2016-09-01T20:19:35.267475"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change '3/31971 to 3/3/1971\n",
    "# change 41/8/1966 to 4/8/1966 \n",
    "all_app3.loc[all_app3.application_date=='3/31971', 'application_date'] = '3/3/1971'\n",
    "all_app3.loc[all_app3.application_date=='41/8/1966', 'application_date'] = '4/8/1966'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:37.098677",
     "start_time": "2016-09-01T20:19:35.833195"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert application date to date object\n",
    "all_app3.loc[:, 'application_date'] = all_app3['application_date'].apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:41.708628",
     "start_time": "2016-09-01T20:19:40.503268"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do the same date check for birth date columns\n",
    "bdate_mask = all_app3.date_of_birth.apply(id_poorlyfmtdates)\n",
    "all_app3.loc[~bdate_mask, ['first_name', 'last_name', 'date_of_birth']]\n",
    "all_app3.loc[(\n",
    "        all_app3.last_name=='Cook') & \n",
    "                  (all_app3.middle_name=='James') & \n",
    "                  (all_app3.first_name.isnull()), 'date_of_birth'] = '1/27/1940'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:19:43.519625",
     "start_time": "2016-09-01T20:19:43.103641"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in NAMES_COLS:\n",
    "    all_app3.loc[:, 'clean_{}'.format(n)] = all_app3[n].apply(clean_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:20:17.093591",
     "start_time": "2016-09-01T20:20:17.060955"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suffix_mask = all_app3.clean_last_name.apply(has_suffix)\n",
    "# all_app_short.loc[suffix_mask, ['clean_last_name', 'clean_first_name', 'clean_middle_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:20:17.828772",
     "start_time": "2016-09-01T20:20:17.711668"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for those last names that seem to have a suffix, pull into seperate column and keep everything last word of last name\n",
    "all_app3.loc[suffix_mask, 'clean_suffix'] = all_app3.loc[suffix_mask, 'clean_last_name'].apply(get_suffix)\n",
    "all_app3.loc[suffix_mask, 'clean_last_name'] = all_app3.loc[suffix_mask, 'clean_last_name'].apply(remove_suffix_from_last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:20:18.312776",
     "start_time": "2016-09-01T20:20:18.294120"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some first names also contain some honors such as 'Pfizer Award' or 'Honor Society'\n",
    "# these should be pulled into the honors and awards columns\n",
    "has_award_fnc = funcy.rpartial(has_award, AWARDS_KEYWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:20:19.286688",
     "start_time": "2016-09-01T20:20:19.169921"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['honor_societies_first', 'honor_societies_fourth', 'honor_societies_second', 'honor_societies_third']\n"
     ]
    }
   ],
   "source": [
    "# get a list of all the med school honors columns\n",
    "honors_columns = [c for c in all_app3.columns if 'honor' in c]\n",
    "print honors_columns\n",
    "\n",
    "has_award_mask = all_app3['clean_first_name'].apply(has_award_fnc)\n",
    "\n",
    "all_app3.loc[has_award_mask, 'extra_honor'] = all_app3.loc[has_award_mask, 'clean_first_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:20:20.986356",
     "start_time": "2016-09-01T20:20:20.809446"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>honor_societies_first</th>\n",
       "      <th>honor_societies_fourth</th>\n",
       "      <th>honor_societies_second</th>\n",
       "      <th>honor_societies_third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [honor_societies_first, honor_societies_fourth, honor_societies_second, honor_societies_third]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create column mask for each row where one of the honors columns is blank\n",
    "for hc in honors_columns:\n",
    "    hc_mask = (has_award_mask) & (pd.isnull(all_app3[hc]))\n",
    "    all_app3.loc[hc_mask, hc] = all_app3.loc[hc_mask, 'extra_honor']\n",
    "# check for any columns that already have full honors and cant be filled\n",
    "all_app3.loc[hc_mask, honors_columns].dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:20:21.743474",
     "start_time": "2016-09-01T20:20:21.719760"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop extra honor columns\n",
    "all_app4 = all_app3.drop('extra_honor', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:22.217008",
     "start_time": "2016-09-01T20:58:22.191216"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace those honors first names with np.nan\n",
    "all_app4.loc[has_award_mask, 'clean_first_name'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:32.185785",
     "start_time": "2016-09-01T20:58:30.926557"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for those columns where med school is equal to last name or med_school is a year range, delete\n",
    "med_school_str_sim = funcy.rpartial(str_sim, 'medical_school', 'clean_last_name')\n",
    "all_app4.loc[:, 'school_name_sim'] = all_app4.loc[:, ['clean_last_name', 'medical_school']].apply(med_school_str_sim, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:32.254744",
     "start_time": "2016-09-01T20:58:32.188128"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>application_date</th>\n",
       "      <th>associate_program_entered</th>\n",
       "      <th>bob</th>\n",
       "      <th>ca</th>\n",
       "      <th>cc</th>\n",
       "      <th>cord</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>...</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>undergrad_year_grad</th>\n",
       "      <th>raw_uuid</th>\n",
       "      <th>flag_missing_app_date</th>\n",
       "      <th>clean_first_name</th>\n",
       "      <th>clean_middle_name</th>\n",
       "      <th>clean_last_name</th>\n",
       "      <th>clean_suffix</th>\n",
       "      <th>school_name_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, address, age, application_date, associate_program_entered, bob, ca, cc, cord, citizenship, city, clinical, dbs, daniel's_comments, date_of_birth, fifth, ic, medical_school, nci, nei, nhi, nhli, niaid, niamd, niamdd, nichd, nichhd, nidr, niehs, nigms, nimh, nindb, ninds, oir, other, pi, pharm_ra, ra, rejected, rejection_date, research, residency_hospital, residency_type, residency_year(s), sa, sixth, state, teaching, undergraduate_school, withdrawal, year_accepted, zip_code, first_name, honor_societies_first, honor_societies_fourth, honor_societies_second, honor_societies_third, internship_hospital_1, internship_year(s), last_name, medschool_year_grad, middle_name, reviewer, undergrad_year_grad, raw_uuid, flag_missing_app_date, clean_first_name, clean_middle_name, clean_last_name, clean_suffix, school_name_sim]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 71 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_app4.loc[all_app4.school_name_sim > .6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:36.241560",
     "start_time": "2016-09-01T20:58:36.176685"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_app4.loc[:, 'clean_college'] = all_app4.undergraduate_school.apply(clean_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_remove_college = [\n",
    "    ' AND ', ' AT ', 'THE ', ' COLLGE', 'UNIVERISTY', 'UNIVERWSITY', 'MASSACHUSSETTS', 'JOHN ', 'DE PAUW', 'ASBURY', \n",
    "'DREXEL INSTITUTE OF TECHNOLOGY', 'A B BROWN UNIVERSITY', 'DARTMOUTH MEDICAL SCHOOL', 'RENSSELAER UNIVERSITY', \n",
    "'RENSSELAER POLYTECHNICAL INSTITUTE', ' STE', 'COLLEGE OF HOLY CROSS', 'HOLLY CROSS', 'JOHNSS ',  'BERKLEY',\n",
    "'UC ', 'PITTSBURRGH', 'WESLYN', 'WILLAMS', 'GEORGIA TECH', 'NEW YORK UNIVERSITY UNIV', \n",
    "'UNIVERSITY OF MICHIGAN IS A', 'OHIO', 'STATE UNIVERSITY OF NEW YORK AT BUFFALO']\n",
    "to_replace_college = [\n",
    "    ' ', ' ', ' ', ' COLLEGE', 'UNIVERSITY', 'UNIVERSITY', 'MASSACHUSETTS', 'JOHNS ', 'DEPAUW', 'ASHBURY',\n",
    "    'DREXEL UNIVERSITY', 'BROWN UNIVERSITY', 'DARTMOUTH', 'RENSSELAER POLYTECHNIC INSTITUTE', \n",
    "    'RENSSELAER POLYTECHNIC INSTITUTE', ' STATE', 'HOLY CROSS', 'HOLY CROSS', 'JOHNS ', \n",
    "    ' BERKELEY', 'UNIVERSITY OF CALIFORNIA ', 'PITTSBURGH', 'WESLEYAN', 'WILLIAMS', \n",
    "    'GEORGIA INSTITUTE OF TECHNOLOGY', 'NEW YORK', 'UNIVERSITY OF MICHIGAN', 'OHIO STATE', 'SUNY BUFFALO']\n",
    "\n",
    "clean_college_fnc = funcy.rpartial(clean_std_college_name, to_remove_college, to_replace_college)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:39.328048",
     "start_time": "2016-09-01T20:58:39.229049"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make college mispelling and different reference translation table\n",
    "all_app4.loc[:, 'clean_college_trans'] = all_app4.clean_college.apply(clean_college_fnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app4.drop(['clean_college', 'school_name_sim'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC BERKELEY\n"
     ]
    }
   ],
   "source": [
    "all_app4.loc[:, 'medical_school'] = all_app4.medical_school.apply(funcy.rcompose(clean_names, clean_med_school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ALABAMA', 'ALBERT EINSTEIN COLLEGE OF MEDICINE OF YESHIVA',\n",
       "       'ARIZONA', 'ARKANSAS', 'BAYLOR', 'BOSTON', 'CASE WESTERN RESERVE',\n",
       "       'CHICAGO', 'CINCINNATI', 'COLORADO', 'COLUMBIA', 'CONNECTICUT',\n",
       "       'DARTMOUTH ', 'DUKE', 'EMORY', 'FLORIDA', 'GEORGE WASHINGTON',\n",
       "       'GEORGETOWN', 'GEORGIA', 'HAHNEMANN', 'HARVARD ', 'HOWARD',\n",
       "       'ILLINOIS', 'INDIANA', 'IOWA',\n",
       "       'JEFFERSON MEDICAL COLLEGE OF THOMAS JEFFERSON',\n",
       "       'JOAN SANFORD I WEILL MEDICAL COLLEGE CORNELL', 'JOHNS HOPKINS',\n",
       "       'KANSAS', 'KENTUCKY', 'LOMA LINDA', 'LOUISIANA STATE', 'LOUISVILLE',\n",
       "       'LOYOLA', 'MARYLAND', 'MIAMI', 'MICHIGAN', 'MICHIGAN STATE',\n",
       "       'MINNESOTA', 'MISSISSIPPI', 'MISSOURI', 'NEBRASKA', 'NEW', 'NORTH',\n",
       "       'NORTHWESTERN', 'NYU', 'OHIO', 'OHIO STATE', 'OKLAHOMA',\n",
       "       'OREGON HEALTH SCIENCES', 'PENNSYLVANIA', 'PENNSYLVANIA STATE',\n",
       "       'PITTSBURGH', 'ROCHESTER', 'SAINT LOUIS', 'SOUTH', 'STANFORD',\n",
       "       'SUNY', 'TEMPLE', 'TENNESSEE', 'TEXAS', 'TUFTS', 'TULANE',\n",
       "       'UC BERKELEY', 'UC DAVIS ', 'UCLA ', 'UCSD ', 'UCSF ',\n",
       "       'UMDNJ NEW JERSEY ', 'USC KECK', 'UTAH', 'VANDERBILT', 'VERMONT',\n",
       "       'VIRGINIA', 'WAKE FOREST', 'WASHINGTON', 'WAYNE STATE',\n",
       "       'WEST VIRGINIA', 'WISCONSIN', 'YALE', nan, None], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_app4.medical_school.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:40.388516",
     "start_time": "2016-09-01T20:58:40.360397"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to convert undergrad_year_grad and med_school_grad to numbers to maintain consistence\n",
    "all_app4.loc[:, ['undergrad_year_grad', 'medschool_year_grad']] = all_app4.loc[:, ['undergrad_year_grad', 'medschool_year_grad']].apply(\n",
    "    lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:58:41.751981",
     "start_time": "2016-09-01T20:58:41.684060"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now, sort by names, med school, undergrad school, \n",
    "all_app5 = all_app4.sort_values(by=PERSONAL_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T20:42:07.602295",
     "start_time": "2016-09-01T20:42:07.581027"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LAST_NAME_MISSPELLINGS = {\n",
    "#         'HOMCY': 'HOMEY', 'DROBIS': 'DROBIN', 'DEFRONZO':'DEFRENZO', \n",
    "#         'BRADEN 3R': 'BRADEN', 'BORKER': 'BORER', 'CASTLES': 'CASTLE',\n",
    "#         'CUONO': 'CUOMO', 'CYRULNIK': 'CYRULINK', 'EISENBATH': 'EISENBARTH', \n",
    "#         'ELLIOTT': 'ELIOT', 'FINKLESTEIN': 'FINKELSTEIN', 'HEINRICK': 'HEINRICH', \n",
    "#         'HERLIKY': 'HERLIHY', 'HIMMELHOCK': 'HIMMELHOCH', 'JANOWSKY': 'JANKOWSKY', \n",
    "#         'KLINENBERG': 'KLINEBERG', 'KORNFELD': 'KORNFIELD', 'NEIDORF': 'NEIDOFT',\n",
    "#         'OLEINICK': 'OLENICK', 'ROSKES': 'ROSKE'\n",
    "# }\n",
    "# removed CUONO, DROBIS, \n",
    "\n",
    "\n",
    "LAST_NAME_MISSPELLINGS = {\n",
    "        'HOMCY': 'HOMEY', \n",
    "        'BRADEN 3R': 'BRADEN', 'BORKER': 'BORER', 'CASTLES': 'CASTLE',\n",
    "        'CYRULNIK': 'CYRULINK', 'EISENBATH': 'EISENBARTH', \n",
    "        'HEINRICK': 'HEINRICH', \n",
    "        'HERLIKY': 'HERLIHY', 'HIMMELHOCK': 'HIMMELHOCH', 'JANOWSKY': 'JANKOWSKY', \n",
    "        'KLINENBERG': 'KLINEBERG', 'KORNFELD': 'KORNFIELD', 'NEIDORF': 'NEIDOFT',\n",
    "        'OLEINICK': 'OLENICK', 'ROSKES': 'ROSKE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replace_last_name_fnc = funcy.rpartial(replace_last_name, LAST_NAME_MISSPELLINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T22:24:26.118320",
     "start_time": "2016-09-01T22:24:26.081797"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct last name mispellings\n",
    "all_app5.loc[:, 'clean_last_name'] = all_app5.loc[:, 'clean_last_name'].apply(replace_last_name_fnc)\n",
    "all_app5.loc[all_app5.clean_last_name=='MORTON', 'clean_first_name'] = 'JOHN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:02:42.117492",
     "start_time": "2016-09-01T21:02:41.859499"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert ca column to float62\n",
    "all_app5.loc[:, 'ca'] = all_app5.loc[:, 'ca'].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:02:42.790493",
     "start_time": "2016-09-01T21:02:42.741902"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sorting_fnc(v):\n",
    "    # if string entry, sort by length, otherwise return value\n",
    "    if isinstance(v, str):\n",
    "        return len(v)\n",
    "    return v\n",
    "\n",
    "def consolidate_holes(df_col):\n",
    "    # for each dataframe of info on one person/application year pair, consolidate info\n",
    "    # drop null values, get unique and take longest by string length or first numeric value\n",
    "    # all dataframes are coming in with reviewer 2 on top, who seems to be more accurate\n",
    "    # so take that answer if possible\n",
    "    lst_vals = list(df_col.dropna().unique())\n",
    "    if len(lst_vals) == 0:\n",
    "        return np.nan\n",
    "    if isinstance(lst_vals[0], str):\n",
    "        lst_vals = sorted(lst_vals, key=sorting_fnc, reverse=True)\n",
    "    elif isinstance(lst_vals[0], np.datetime64):\n",
    "        print lst_vals\n",
    "        # if multiple dates of application, choose latest\n",
    "        lst_vals = sorted(lst_vals, reverse=True)\n",
    "    return lst_vals[0]\n",
    "\n",
    "def _stringify_info(val_series):\n",
    "    # accepts a series, returns a string\n",
    "    str_vals = [str(v) for v in val_series]\n",
    "    return '_'.join(str_vals)\n",
    "\n",
    "def stringify_personal_info(df_row):\n",
    "    # accepts a series, returns a list object\n",
    "    val_series = df_row[['clean_first_name', 'clean_middle_name', \n",
    "                        'clean_last_name', 'clean_college_trans', 'medical_school', 'date_of_birth']]\n",
    "    return _stringify_info(val_series)\n",
    "\n",
    "def stringify_ids(df_row):\n",
    "    # accepts a series, returns a list object\n",
    "    # takes the raw id card row of consolidated cols and turns it into a string to save\n",
    "    val_series = df_row[RAW_CARD_ID]\n",
    "    return _stringify_info(val_series)\n",
    "\n",
    "def add_sanity_check_row(df, vals=None):\n",
    "    if vals is None:\n",
    "        df.loc[:, 'sanity_check'] = np.nan\n",
    "    else:\n",
    "        df.loc[:, 'sanity_check'] = vals\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_merged_ids(df, vals=None):\n",
    "    # add combined id columns\n",
    "    # add an id column that track what is the index of columns consolidated into one\n",
    "    if vals is None:\n",
    "        df.loc[:, RAW_INDEX_IDS] = np.nan\n",
    "    else:\n",
    "        df.loc[:, RAW_INDEX_IDS] = vals\n",
    "    return df\n",
    "    \n",
    "\n",
    "def format_consolidated_data(df):\n",
    "    # applies consolidate fnc to dataframe, converts it into a df that can be merged\n",
    "    # sort df so reviewer 2 is on top\n",
    "    df_sort = df.sort_values(by=['reviewer'], ascending=False)\n",
    "    vals = map(lambda x: stringify_personal_info(df_sort.loc[x, :]), df_sort.index)\n",
    "    d2_series = df_sort.apply(consolidate_holes)\n",
    "    df_trans = pd.DataFrame(d2_series).T\n",
    "    # create a column that compares string values in another column\n",
    "    str_ids = stringify_ids(df)\n",
    "    df_trans2 = add_merged_ids(df_trans, str_ids)\n",
    "    sanity_checks = '\\n'.join(vals)\n",
    "    return add_sanity_check_row(df_trans2, sanity_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VERY IMPORTANT, NEED TO RESET THE INDEX\n",
    "all_app6 = all_app5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app6.loc[:, 'application_year'] = pd.DatetimeIndex(all_app6.application_date).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_app6.sort_values('clean_last_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:02:45.503298",
     "start_time": "2016-09-01T21:02:45.482905"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_name_grouped = all_app6.groupby('clean_last_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del all_app3, all_app4, all_app5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:02:46.942074",
     "start_time": "2016-09-01T21:02:46.891554"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to consolidate multiple rows into one person\n",
    "def consolidate_person(candidate_df):\n",
    "    # for each dataframe, with a common last name, seperate into unique people\n",
    "    # easiest case, if 2 rows, 1 from reviewer 1 and 2, then consolidate\n",
    "    df_len = candidate_df.shape[0]\n",
    "    if df_len < 2:\n",
    "        # if only 1 row in data group, return row\n",
    "        cd2 = add_sanity_check_row(candidate_df)\n",
    "        # create a column that compares string values in another column\n",
    "        str_ids = stringify_ids(candidate_df)\n",
    "        return add_merged_ids(cd2, str_ids)\n",
    "    unique_first_names = candidate_df['clean_first_name'].dropna().unique()\n",
    "    unique_undergrad = candidate_df['undergrad_year_grad'].dropna().unique()\n",
    "    unique_college = candidate_df['clean_college_trans'].dropna().unique()\n",
    "    if (len(unique_first_names) < 2 and len(unique_undergrad) < 2) or (len(unique_first_names) < 2 and len(unique_college) < 2):\n",
    "        # if unique first names < 2 and < 2 diff years of undergrad OR \n",
    "        # unique first names < 2 and <2 unique college names\n",
    "        # most likely this is the same person\n",
    "        return format_consolidated_data(candidate_df)\n",
    "    # otherwise there are more than 1 person to combine\n",
    "    if len(unique_first_names) >= 2:\n",
    "        # if there are 2 or more different first names, try to group by first and last name\n",
    "        # and then analyze each group seperately\n",
    "        new_grped = candidate_df.groupby(['clean_first_name', 'clean_last_name'])\n",
    "        res = map(lambda (x, y): format_consolidated_data(y), new_grped)\n",
    "        return pd.concat(res)\n",
    "    if len(unique_college) >= 2:\n",
    "        # if two different colleges, try to group by last name and college name\n",
    "        new_grped = candidate_df.groupby(['clean_first_name', 'clean_college_trans'])\n",
    "        res = map(lambda (x, y): format_consolidated_data(y), new_grped)\n",
    "        return pd.concat(res)\n",
    "    # otherise this is an edge case\n",
    "    # otherwise, although the person doesn't match on unique first and undergrad years, they are still most likely one \n",
    "    # person\n",
    "    print candidate_df\n",
    "    raise AttributeError('Hitting a Bad Edge Case')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:07:56.910111",
     "start_time": "2016-09-01T21:02:47.824198"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_rows = map(lambda (x, y): consolidate_person(y), last_name_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:08:07.950045",
     "start_time": "2016-09-01T21:07:56.912108"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_people_df = pd.concat(combined_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset index\n",
    "unique_people_df = unique_people_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-01T21:08:08.003521",
     "start_time": "2016-09-01T21:08:07.952632"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_people_df.sort_values(by=['clean_last_name'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_people_df[PERSON_ID] = unique_people_df.clean_last_name.apply(lambda x: str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>application_date</th>\n",
       "      <th>application_year</th>\n",
       "      <th>associate_program_entered</th>\n",
       "      <th>bob</th>\n",
       "      <th>ca</th>\n",
       "      <th>cc</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>...</th>\n",
       "      <th>sanity_check</th>\n",
       "      <th>sixth</th>\n",
       "      <th>state</th>\n",
       "      <th>teaching</th>\n",
       "      <th>undergrad_year_grad</th>\n",
       "      <th>undergraduate_school</th>\n",
       "      <th>withdrawal</th>\n",
       "      <th>year_accepted</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>person_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, address, age, application_date, application_year, associate_program_entered, bob, ca, cc, citizenship, city, clean_college_trans, clean_first_name, clean_last_name, clean_middle_name, clean_suffix, clinical, cord, daniel's_comments, date_of_birth, dbs, fifth, first_name, flag_missing_app_date, honor_societies_first, honor_societies_fourth, honor_societies_second, honor_societies_third, ic, internship_hospital_1, internship_year(s), last_name, medical_school, medical_school2, medschool_year_grad, middle_name, nci, nei, nhi, nhli, niaid, niamd, niamdd, nichd, nichhd, nidr, niehs, nigms, nimh, nindb, ninds, oir, other, pharm_ra, pi, ra, raw_card_ids, raw_uuid, rejected, rejection_date, research, residency_hospital, residency_type, residency_year(s), reviewer, sa, sanity_check, sixth, state, teaching, undergrad_year_grad, undergraduate_school, withdrawal, year_accepted, zip_code, person_uuid]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 76 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to make sure no uuid dups\n",
    "unique_people_df.sort_values(PERSON_ID, inplace=True)\n",
    "unique_people_df[unique_people_df.duplicated(PERSON_ID, keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'address',\n",
       " 'age',\n",
       " 'application_date',\n",
       " 'application_year',\n",
       " 'associate_program_entered',\n",
       " 'bob',\n",
       " 'ca',\n",
       " 'cc',\n",
       " 'citizenship',\n",
       " 'city',\n",
       " 'clean_college_trans',\n",
       " 'clean_first_name',\n",
       " 'clean_last_name',\n",
       " 'clean_middle_name',\n",
       " 'clean_suffix',\n",
       " 'clinical',\n",
       " 'cord',\n",
       " 'date_of_birth',\n",
       " 'dbs',\n",
       " 'fifth',\n",
       " 'first_name',\n",
       " 'flag_missing_app_date',\n",
       " 'honor_societies_first',\n",
       " 'honor_societies_fourth',\n",
       " 'honor_societies_second',\n",
       " 'honor_societies_third',\n",
       " 'ic',\n",
       " 'internship_hospital_1',\n",
       " 'internship_year(s)',\n",
       " 'last_name',\n",
       " 'medical_school',\n",
       " 'medical_school2',\n",
       " 'medschool_year_grad',\n",
       " 'middle_name',\n",
       " 'nci',\n",
       " 'nei',\n",
       " 'nhi',\n",
       " 'nhli',\n",
       " 'niaid',\n",
       " 'niamd',\n",
       " 'niamdd',\n",
       " 'nichd',\n",
       " 'nichhd',\n",
       " 'nidr',\n",
       " 'niehs',\n",
       " 'nigms',\n",
       " 'nimh',\n",
       " 'nindb',\n",
       " 'ninds',\n",
       " 'oir',\n",
       " 'person_uuid',\n",
       " 'pharm_ra',\n",
       " 'pi',\n",
       " 'ra',\n",
       " 'raw_card_ids',\n",
       " 'raw_uuid',\n",
       " 'rejected',\n",
       " 'rejection_date',\n",
       " 'research',\n",
       " 'residency_hospital',\n",
       " 'residency_type',\n",
       " 'residency_year(s)',\n",
       " 'reviewer',\n",
       " 'sa',\n",
       " 'sanity_check',\n",
       " 'sixth',\n",
       " 'state',\n",
       " 'teaching',\n",
       " 'undergrad_year_grad',\n",
       " 'undergraduate_school',\n",
       " 'withdrawal',\n",
       " 'year_accepted',\n",
       " 'zip_code']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(unique_people_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_people_df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2329    None\n",
       "2141    None\n",
       "3781    None\n",
       "3724    None\n",
       "1718    None\n",
       "3433    None\n",
       "3125    None\n",
       "3046    None\n",
       "921     None\n",
       "1014    None\n",
       "525     None\n",
       "3237    None\n",
       "724     None\n",
       "2913    None\n",
       "483     None\n",
       "778     None\n",
       "2222    None\n",
       "474     None\n",
       "2888    None\n",
       "3789    None\n",
       "713     None\n",
       "2771    None\n",
       "3100    None\n",
       "1016    None\n",
       "497     None\n",
       "2877    None\n",
       "2049    None\n",
       "2930    None\n",
       "209     None\n",
       "1461    None\n",
       "        ... \n",
       "563     None\n",
       "2432    None\n",
       "1374    None\n",
       "83      None\n",
       "918     None\n",
       "2815    None\n",
       "3655    None\n",
       "692     None\n",
       "2476    None\n",
       "2656    None\n",
       "2133    None\n",
       "3723    None\n",
       "753     None\n",
       "2445    None\n",
       "1997    None\n",
       "3025    None\n",
       "651     None\n",
       "2663    None\n",
       "1928    None\n",
       "1686    None\n",
       "2501    None\n",
       "3984    None\n",
       "2609    None\n",
       "1840    None\n",
       "1586    None\n",
       "859     None\n",
       "2848    None\n",
       "3694    None\n",
       "1833    None\n",
       "2174    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge uuid back into person, app date data set\n",
    "def update_person_uuid(indexs, g_uuid, df):\n",
    "    # take the stringified information stored in the 'raw_index_ids' column,\n",
    "    # which should hold the raw card ids related to each unique person\n",
    "    # go to places in the data set with that location and update the uuid column\n",
    "    if pd.isnull(indexs):\n",
    "        return np.nan\n",
    "    if isinstance(indexs, str):\n",
    "        inds = indexs.split('_')\n",
    "    else:\n",
    "        raise TypeError('Unkown id type')\n",
    "    df.loc[df[RAW_CARD_ID].isin(inds), PERSON_ID] = g_uuid\n",
    "        \n",
    "# note this is a gross mutation operation! Im mutating the ids column in all_app5 to add in the personal id of all people \n",
    "# matched into one column\n",
    "unique_people_df[[RAW_INDEX_IDS, PERSON_ID]].apply(lambda x: update_person_uuid(x[RAW_INDEX_IDS], x[PERSON_ID], all_app6), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_app6.sort_values([PERSON_ID, 'clean_last_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_people_df.loc[:, 'application_year'] = pd.DatetimeIndex(unique_people_df.application_date).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app6.loc[pd.isnull(all_app6[PERSON_ID]), 'not_matched'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_match(row, to_search):\n",
    "    matching_rows = to_search.loc[\n",
    "        (to_search.clean_last_name==row['clean_last_name']) & (\n",
    "            to_search.application_date==row['application_date']) & \n",
    "        (to_search.city==row['city']), :]\n",
    "    if matching_rows.shape[0] == 0:\n",
    "         return np.nan\n",
    "    if matching_rows.shape[0] == 1:\n",
    "        return matching_rows[PERSON_ID].values[0]\n",
    "    raise AttributeError('Multiple Matches')\n",
    "    print row[['clean_last_name', 'city', 'clean_first_name', 'application_date']]\n",
    "    print matching_rows[['clean_last_name', 'city', 'clean_first_name', 'application_date']]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each not matched person, search for a name, application date\n",
    "all_app6.loc[all_app6.not_matched==1, PERSON_ID] = all_app6.loc[all_app6.not_matched==1, :].apply(lambda x: find_match(x, unique_people_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_extra_match_ids(row, to_search):\n",
    "    to_add_ind = to_search.loc[to_search[PERSON_ID]==row[PERSON_ID], :][RAW_CARD_ID].values[0]\n",
    "    existing = row[RAW_INDEX_IDS]\n",
    "    if pd.isnull(existing):\n",
    "        return str(to_add_ind)\n",
    "    return '{0}_{1}'.format(existing, str(to_add_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update unique df ids columns with extra info\n",
    "not_matched_people = all_app6.loc[all_app6.not_matched==1, :]\n",
    "\n",
    "unique_people_df.loc[unique_people_df[PERSON_ID].isin(not_matched_people[PERSON_ID].dropna()), 'extra_ids'] = \\\n",
    "    unique_people_df.loc[unique_people_df[PERSON_ID].isin(not_matched_people[PERSON_ID].dropna()), :].apply(\n",
    "        lambda x: add_extra_match_ids(x, not_matched_people), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace ids columns with extra ids where applicable\n",
    "unique_people_df.loc[~pd.isnull(unique_people_df.extra_ids), RAW_INDEX_IDS] = unique_people_df.loc[~pd.isnull(\n",
    "        unique_people_df.extra_ids), 'extra_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, I want to group all_app5 by personal uuid and then find people with two different application dates\n",
    "all_apps_grouped = all_app6.groupby(PERSON_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_app6.loc[all_app6.duplicated('uuid', keep=False), PERSONAL_INFO+['sanity_check','uuid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for people in a group, flag those with multiple app dates\n",
    "# flag those whose med schools don't match\n",
    "mismatch_med_school = []\n",
    "for g, items in all_apps_grouped:\n",
    "    unique_ms = items['medical_school'].dropna().unique()\n",
    "    if len(unique_ms) > 1:\n",
    "        mismatch_med_school.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_app6.loc[all_app6[PERSON_ID].isin(mismatch_med_school), ['clean_last_name', 'clean_first_name', 'medical_school']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each group, if med school and city are not close at all for string sim, add person to the unique data set\n",
    "# func should output rows to add to unique data set or None\n",
    "def compute_city_sim(row, main_app_year, main_city):\n",
    "    if row['application_year'] != main_app_year or pd.isnull(row['city']):\n",
    "        return np.nan\n",
    "    return difflib.SequenceMatcher(None, main_city, row['city']).ratio()\n",
    "\n",
    "\n",
    "\n",
    "def reconcile_med_school(group_to_reconcile):\n",
    "    g_uuid = group_to_reconcile[PERSON_ID].unique()[0]\n",
    "    main_ms = unique_people_df.loc[unique_people_df[PERSON_ID]==g_uuid, 'medical_school'].unique()[0]\n",
    "    main_app_year = unique_people_df.loc[unique_people_df[PERSON_ID]==g_uuid, 'application_year'].unique()[0]\n",
    "    main_college =  unique_people_df.loc[unique_people_df[PERSON_ID]==g_uuid, 'clean_college_trans'].unique()[0]\n",
    "    main_city = unique_people_df.loc[unique_people_df[PERSON_ID]==g_uuid, 'city'].unique()[0]\n",
    "    mismatched = group_to_reconcile.loc[\n",
    "        (group_to_reconcile.medical_school!=main_ms) & (~pd.isnull(group_to_reconcile.medical_school)), :]\n",
    "    mismatched.loc[:, 'med_school_sim'] = mismatched.medical_school.apply(\n",
    "        lambda x: difflib.SequenceMatcher(None, main_ms, x).ratio())\n",
    "    mismatched.loc[:, 'city_sim'] = mismatched.apply(lambda x: compute_city_sim(x, main_app_year, main_city), axis=1)\n",
    "    mismatched.loc[:, 'app_year_diff'] = mismatched.application_year.apply(lambda x: abs(main_app_year-x))\n",
    "    return mismatched.loc[\n",
    "        (mismatched.med_school_sim < .8) | ((\n",
    "                ~pd.isnull(mismatched.city_sim)) & (mismatched.city_sim < .8)) | (\n",
    "            mismatched.app_year_diff > 3), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mismatched_row(g_uuid, to_select):\n",
    "    g = to_select.loc[to_select[PERSON_ID]==g_uuid, :]\n",
    "    return reconcile_med_school(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_unique = pd.concat(map(lambda x: find_mismatched_row(x, all_app6), mismatch_med_school), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "# for each of the rows accidentally grouped, find in data set, change uuid and add new row to unique_df with ids col\n",
    "all_app6.loc[all_app6.index.isin(to_unique.index), 'unique_flag'] = 1\n",
    "print len(to_unique.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app6.loc[all_app6.unique_flag==1, 'uuid2'] = all_app6.apply(lambda x: str(uuid.uuid4()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app6.loc[all_app6.unique_flag==1, PERSON_ID] = all_app6.loc[all_app6.unique_flag==1, 'uuid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_add = all_app6.loc[all_app6.unique_flag==1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# overwrite raw data set column with the raw card id of the row to add to unique data set\n",
    "to_add.loc[:, RAW_INDEX_IDS] = to_add[RAW_CARD_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del all_app6['uuid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_people2 = unique_people_df.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4167, 80)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_people2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_g2 = all_app6.groupby(PERSON_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiple_apps = []\n",
    "mismatch_med_school = []\n",
    "for g, items in all_g2:\n",
    "    unique_apps = items['application_date'].dropna().unique()\n",
    "    if len(unique_apps) > 1:\n",
    "        multiple_apps.append(g)\n",
    "    unique_ms = items['medical_school'].dropna().unique()\n",
    "    if len(unique_ms) > 1:\n",
    "        mismatch_med_school.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mismatch_med_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multiple_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app7 = all_app6.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app8 = all_app7.rename(columns={'index': PERSON_APPLICATION_ID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop application dates prior to 1950\n",
    "all_app8 = all_app8[(all_app8.application_year > 1950) & (all_app8.application_year < 1976)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_app9 = all_app8.drop_duplicates([PERSON_ID, 'application_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_year_accepted(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app9.loc[:, 'flag_rejected'] = all_app9.apply(\n",
    "    lambda x: 1 if not pd.isnull(x['rejection_date']) or x['rejected']==1 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app9.loc[:, 'year_accepted'] = all_app9.year_accepted.apply(convert_year_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# now, we want to make data set wide- add multiple applications horizontally\n",
    "all_app9.sort_values([PERSON_ID, 'application_year'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# at most 3 application dates\n",
    "grouped_ppl = all_app9.groupby(PERSON_ID)\n",
    "multiple_apps = []\n",
    "\n",
    "for key, df in grouped_ppl:\n",
    "    if df['application_date'].shape[0] == 2:\n",
    "        multiple_apps.append(\n",
    "            pd.DataFrame(\n",
    "            {PERSON_ID: [df.iloc[1][PERSON_ID]], 'application_date_2': [df.iloc[1]['application_date']]}))\n",
    "    elif df['application_date'].shape[0] == 3:\n",
    "        multiple_apps.append(pd.DataFrame(\n",
    "            {PERSON_ID: [df.iloc[1][PERSON_ID]], 'application_date_2': [df.iloc[1]['application_date']], \n",
    "            'application_date_3': [df.iloc[2]['application_date']]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_app_dates = pd.concat(multiple_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app10 = pd.merge(left=all_app9, right=add_app_dates, left_on=PERSON_ID, right_on=PERSON_ID, how='left')\n",
    "unique_people3 = pd.merge(left=unique_people2, right=add_app_dates, left_on=PERSON_ID, right_on=PERSON_ID, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app11 = all_app10.drop(['Unnamed: 0', \"daniel's_comments\", 'other',  'fifth', 'sixth', 'unique_flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app9 = all_app9.drop(['Unnamed: 0', \"daniel's_comments\", 'other',  'fifth', 'sixth', 'unique_flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_people4 = unique_people3.drop([\n",
    "        'Unnamed: 0', \"daniel's_comments\", 'other',  'fifth', 'sixth', \n",
    "        'unique_flag', 'extra_ids', 'medical_school2', 'uuid2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_app9.sort_values(['clean_last_name', 'application_year']).to_pickle(os.path.join(APP_DATA_DIR, 'person_application_date.p'))\n",
    "all_app9.sort_values(['clean_last_name', 'application_year']).to_csv(os.path.join(APP_DATA_DIR, 'person_application_date.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_app11.sort_values(['clean_last_name', 'application_year']).to_pickle(os.path.join(APP_DATA_DIR, 'person_application_date_wide.p'))\n",
    "all_app11.to_csv(os.path.join(APP_DATA_DIR, 'person_application_date_wide.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_people3.to_csv(os.path.join(APP_DATA_DIR, 'unique_applicants.csv'))\n",
    "unique_people3.to_pickle(os.path.join(APP_DATA_DIR, 'unique_applicants.p'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
