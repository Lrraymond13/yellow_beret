{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in known applicant files, clean and try to dedupe\n",
    "from collections import Counter\n",
    "import difflib\n",
    "from fuzzywuzzy import fuzz\n",
    "import uuid\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import funcy\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "from data_cleaning_functions import correct_mispellings, long_form_date, clean_names, clean_med_school\n",
    "from dev import (\n",
    "    SUFFIXES, FEMALE_FIRST_NAMES, FEMALE_MIDDLE_NAMES, NAME_COLS, RAW_ATT_DATA_DIR, ATT_DATA_DIR, PICKLE_DIR, \n",
    "        CORRECTIONS_DIR, SUM_STAT_DIR)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r1_file = '1964-1973 associates.XLS'\n",
    "r2_file = 'Associates alpha by institute.XLS'\n",
    "r3_file = 'Associates data.XLS'\n",
    "r4_file = 'NIMH Associates Complete.XLS'\n",
    "r5_file = 'NINDB Associates alpha by year.xls'\n",
    "filenames = [r1_file, r2_file, r3_file, r4_file, r5_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "file_df = map(lambda x: pd.read_excel(os.path.join(RAW_ATT_DATA_DIR, x)), filenames)\n",
    "\n",
    "file_4_columns = [\n",
    "    'dno', 'source', 'unknown', 'lastname', 'first_middle', 'institute', 'lab_brch', \n",
    "    'program', 'supervisor', 'eod_year', 'med_school', 'year_grad', 'intern_hos', 'intern_dte',\n",
    "       'res_hosp', 'residency', 'res_dtes'\n",
    "]\n",
    "\n",
    "file_df[2].rename(columns={'lname':'lastname', 'fname': 'first_middle'}, inplace=True)\n",
    "file_df[1].rename(columns={'lname':'lastname', 'fname': 'first_middle'}, inplace=True)\n",
    "\n",
    "file_df[4].columns = file_4_columns\n",
    "# for each files in the list, add a column to track source\n",
    "for name, f in zip(filenames, file_df):\n",
    "    f.loc[:, 'data_source'] = name\n",
    "concat_df = pd.concat(file_df).reset_index(drop=True)\n",
    "\n",
    "print sum(map(lambda x: x.shape[0], file_df)) == concat_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_suffix(lst_strs):\n",
    "    # check for any existing suffix\n",
    "    existing_suffixes = filter(None, filter(lambda x: x in lst_strs, SUFFIXES))\n",
    "    # if suffix is I or V, only return suffix if len of lst_string > 2 (otherwise middle name)\n",
    "    if ('I' in existing_suffixes or 'V' in existing_suffixes) and len(lst_strs) > 2:\n",
    "        return existing_suffixes\n",
    "    return funcy.remove(lambda x: x in ['I', 'V', 'DR'], existing_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_first_middle(raw_str):\n",
    "    # seperate first_middle column string (Thomas, Bruce) into THOMAS BRUCE\n",
    "    # suffix also appears to be in column THOMAS BRUCE JR\n",
    "    names = filter(None, raw_str.split(' '))\n",
    "    cleaned_vals = map(clean_names, names)\n",
    "    suff = has_suffix(cleaned_vals)\n",
    "    # suffix is actually a list\n",
    "    if suff:\n",
    "        root = funcy.remove(lambda x: x==suff[-1], cleaned_vals)\n",
    "        return pd.Series(\n",
    "            {'clean_first_name': root[0],\n",
    "                 'clean_middle_name': ' '.join(root[1:]),\n",
    "                    'clean_suffix': suff[-1]})\n",
    "    return pd.Series(\n",
    "        {'clean_first_name': cleaned_vals[0],\n",
    "             'clean_middle_name': np.nan if len(cleaned_vals)==1 else ' '.join(cleaned_vals[1:]),\n",
    "                'clean_suffix': np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_suffix_df = pd.DataFrame(concat_df.loc[~pd.isnull(concat_df.first_middle), 'first_middle'].apply(strip_first_middle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suffix(raw_str):\n",
    "    # wrapper around get suffix\n",
    "    if pd.isnull(raw_str) or len(raw_str.split(' ')) < 2:\n",
    "        return np.nan\n",
    "    str_list = raw_str.split(' ')\n",
    "    suff = has_suffix(str_list)\n",
    "    if suff:\n",
    "        return suff[-1]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we have all the associates, sep first middle into first and middle name, then sort and check \n",
    "# to see if we have any duplicates\n",
    "df2 = pd.concat([concat_df, cleaned_suffix_df], axis=1)\n",
    "df2.head()\n",
    "df2.loc[:, 'clean_last_name'] = df2.lastname.apply(clean_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10729, 24)\n",
      "(10723, 24)\n"
     ]
    }
   ],
   "source": [
    "# consolidate firstname columns\n",
    "missing_first_middle = pd.isnull(df2.first_middle)\n",
    "df2.loc[missing_first_middle, ['clean_first_name', 'clean_middle_name']] = df2.loc[missing_first_middle, ['firstname', 'middlename']].applymap(clean_names)\n",
    "df2.loc[:, 'len_middle'] = df2.clean_middle_name.apply(lambda x: np.nan if pd.isnull(x) else len(x.split(' ')))\n",
    "\n",
    "# clean suffix out from any other columns\n",
    "may_have_suffix_mask = df2.len_middle > 1\n",
    "df2.loc[may_have_suffix_mask, 'clean_suffix'] = df2[may_have_suffix_mask]['clean_middle_name'].apply(get_suffix)\n",
    "\n",
    "df3 = df2.drop(['first_middle', 'firstname', 'middlename', 'lastname', 'len_middle'], axis=1)\n",
    "# df3 = df2\n",
    "# # dropnow where both first and last name are missing\n",
    "df4 = df3.dropna(subset=['clean_first_name', 'clean_last_name'], how='all')\n",
    "print df3.shape\n",
    "print df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove all females\n",
    "female_mask = np.logical_or(\n",
    "    df4.clean_first_name.isin(FEMALE_FIRST_NAMES), df4.clean_middle_name.isin(FEMALE_MIDDLE_NAMES))\n",
    "\n",
    "df5 = df4.loc[~female_mask, :].sort_values(NAME_COLS)\n",
    "df5.loc[:, 'clean_middle_initial'] = df5.clean_middle_name.apply(lambda x: np.nan if pd.isnull(x) or x=='' else x[0])\n",
    "df5.loc[:, 'clean_first_initial'] = df5.clean_first_name.apply(lambda x: np.nan if pd.isnull(x) or x=='' else x[0])\n",
    "df5.loc[:, 'clean_medical_school'] = df5.med_school.apply(funcy.rcompose(clean_names, clean_med_school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_missing(row):\n",
    "    # count the number of entries that are null\n",
    "    return row[pd.isnull(row)].size\n",
    "\n",
    "df5.loc[:, 'count_missing'] = df5[\n",
    "    NAME_COLS+['clean_medical_school', 'eod_year', 'res_dtes', 'res_hosp', 'year_grad']].apply(count_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10050, 28)\n",
      "(3872, 28)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates by dno\n",
    "df6 = df5.drop_duplicates('dno').sort_values(\n",
    "    ['clean_last_name', 'clean_first_initial', 'clean_middle_name', 'count_missing'], axis=0)\n",
    "print df5.shape\n",
    "print df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_first_name</th>\n",
       "      <th>clean_middle_name</th>\n",
       "      <th>clean_last_name</th>\n",
       "      <th>clean_medical_school</th>\n",
       "      <th>eod_year</th>\n",
       "      <th>res_dtes</th>\n",
       "      <th>res_hosp</th>\n",
       "      <th>year_grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>JOHN</td>\n",
       "      <td>B</td>\n",
       "      <td>HARLEY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clean_first_name clean_middle_name clean_last_name clean_medical_school  \\\n",
       "3979             JOHN                 B          HARLEY                  NaN   \n",
       "\n",
       "      eod_year res_dtes res_hosp  year_grad  \n",
       "3979       NaN      NaN      NaN        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.loc[\n",
    "    df6.clean_last_name=='HARLEY', NAME_COLS+['clean_medical_school', 'eod_year', 'res_dtes', 'res_hosp', 'year_grad']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_year_diff(x):\n",
    "    # get reference value\n",
    "    ref_year = x.iloc[0]\n",
    "    return pd.Series(x - ref_year)\n",
    "\n",
    "def is_duplicate(grouped_df):\n",
    "    # return true if grouped df should be counted as one duplicate, returns filter mask\n",
    "    # check if med schools match/are missing\n",
    "    # check if eod years match/are missing\n",
    "    # first group_id\n",
    "    if grouped_df.shape[0] == 1:\n",
    "        return pd.Series([np.nan])\n",
    "    med_schools = grouped_df['clean_medical_school'].dropna().unique()\n",
    "    # if only <= 1 unique med school, mark as duplicate\n",
    "    ref_dno = grouped_df['dno'].values[0]\n",
    "    if len(med_schools) < 2:\n",
    "        return pd.Series([ref_dno]*grouped_df.shape[0])\n",
    "    ms1 = med_schools[0]\n",
    "    # otherwise, check the string similarity of the med school\n",
    "    med_school_sims = [\n",
    "        np.nan if pd.isnull(ms) else fuzz.partial_token_sort_ratio(\n",
    "                ms1, ms) for ms in grouped_df.clean_medical_school.values]\n",
    "    # for med schools with high string sim, count as dups, otherwise not \n",
    "    is_dups = [ref_dno if pd.isnull(sim) or sim > .8 else False for sim in med_school_sims]\n",
    "    return pd.Series(is_dups)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citizenship</th>\n",
       "      <th>data_source</th>\n",
       "      <th>dno</th>\n",
       "      <th>dob</th>\n",
       "      <th>eod_year</th>\n",
       "      <th>generation</th>\n",
       "      <th>institute</th>\n",
       "      <th>intern_dte</th>\n",
       "      <th>intern_hos</th>\n",
       "      <th>lab_brch</th>\n",
       "      <th>...</th>\n",
       "      <th>unknown</th>\n",
       "      <th>year_grad</th>\n",
       "      <th>clean_first_name</th>\n",
       "      <th>clean_middle_name</th>\n",
       "      <th>clean_suffix</th>\n",
       "      <th>clean_last_name</th>\n",
       "      <th>clean_middle_initial</th>\n",
       "      <th>clean_first_initial</th>\n",
       "      <th>clean_medical_school</th>\n",
       "      <th>count_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Associates alpha by institute.XLS</td>\n",
       "      <td>4130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NIMH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>JUSTIN</td>\n",
       "      <td>ALLEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZIVIN</td>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>NORTHWESTERN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Associates alpha by institute.XLS</td>\n",
       "      <td>4131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NIMH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>JUSTIN</td>\n",
       "      <td>ALLEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZIVIN</td>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>NORTHWESTERN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     citizenship                        data_source   dno  dob  eod_year  \\\n",
       "5556         NaN  Associates alpha by institute.XLS  4130  NaN    1973.0   \n",
       "5557         NaN  Associates alpha by institute.XLS  4131  NaN       NaN   \n",
       "\n",
       "     generation institute intern_dte intern_hos lab_brch      ...        \\\n",
       "5556        NaN      NIMH        NaN        NaN      NaN      ...         \n",
       "5557        NaN      NIMH        NaN        NaN      NaN      ...         \n",
       "\n",
       "     unknown year_grad clean_first_name clean_middle_name clean_suffix  \\\n",
       "5556     NaN    1970.0           JUSTIN             ALLEN          NaN   \n",
       "5557     NaN    1970.0           JUSTIN             ALLEN          NaN   \n",
       "\n",
       "     clean_last_name  clean_middle_initial clean_first_initial  \\\n",
       "5556           ZIVIN                     A                   J   \n",
       "5557           ZIVIN                     A                   J   \n",
       "\n",
       "     clean_medical_school  count_missing  \n",
       "5556         NORTHWESTERN              2  \n",
       "5557         NORTHWESTERN              3  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.loc[df6.dno==4130, :]\n",
    "df6.loc[df6.clean_last_name=='ZIVIN', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df6_grouped = df6.groupby(['clean_last_name', 'clean_first_initial'])\n",
    "df6['eod_year_diff'] = df6_grouped.eod_year.apply(get_year_diff)\n",
    "# df6['is_dup_index'] = df6_grouped.apply(is_duplicate)\n",
    "new_col = df6_grouped.apply(is_duplicate)\n",
    "# reset so no longer grouped as a multi index, then merge in via clean first, last_name\n",
    "\n",
    "# dups = df6_grouped.filter(lambda x: is_duplicate(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3773,)\n",
      "(3773, 3)\n"
     ]
    }
   ],
   "source": [
    "new_col2 = new_col.reset_index()\n",
    "new_col2.drop('level_2', axis=1, inplace=True)\n",
    "print new_col.shape\n",
    "print new_col2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_col2.columns = ['clean_last_name', 'clean_first_initial', 'duplicate_dno']\n",
    "new_col3 = new_col2.drop_duplicates(['clean_last_name', 'clean_first_initial', 'duplicate_dno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_last_name</th>\n",
       "      <th>clean_first_initial</th>\n",
       "      <th>duplicate_dno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_last_name clean_first_initial duplicate_dno\n",
       "0           AARON                   R           NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col4 = new_col3[new_col3['duplicate_dno']!=False]\n",
    "new_col4[new_col4['clean_last_name']=='AARON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7 = pd.merge(left=df6, right=new_col4, on=['clean_last_name', 'clean_first_initial'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7.loc[:, 'eod_year_diff'] = df7.eod_year_diff.abs()\n",
    "df7.loc[(df7.eod_year_diff>2) , 'duplicate_dno'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7.loc[:, 'dup_flag'] = 0\n",
    "dup_mask = df7.dno.isin(df7.duplicate_dno.dropna())\n",
    "df7.loc[dup_mask, 'dup_flag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_flags = ['missing_{}'.format(f) for f in df7.columns]\n",
    "\n",
    "def count_missing(x):\n",
    "    # returns 1 for missing value\n",
    "    if x is None or x in ['None', '', ' ']:\n",
    "        return 1\n",
    "    return 1 if pd.isnull(x) else 0\n",
    "\n",
    "# calculate missing info stats\n",
    "df7[missing_flags] = df7.applymap(count_missing)\n",
    "\n",
    "# calculate missing stats for people in the data set\n",
    "df7[missing_flags].describe().to_csv(os.path.join(SUM_STAT_DIR, 'NIH_attendees_data_fill_rates.csv'))\n",
    "df8 = df7.drop(missing_flags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def consolidate_row(series):\n",
    "    nonnulls = series.dropna().unique()\n",
    "    if nonnulls.shape[0] == 0:\n",
    "        return pd.Series([np.nan]*series.shape[0])\n",
    "    if isinstance(nonnulls[0], str):\n",
    "        val = sorted(nonnulls, key=len, reverse=True)[0]\n",
    "        return pd.Series([val]*series.shape[0])\n",
    "    val = sorted(nonnulls, reverse=True)[0]\n",
    "    return pd.Series([val]*series.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_dups = df8.loc[\n",
    "    df8.duplicated(['clean_last_name', 'clean_first_initial'], keep=False), :].groupby(\n",
    "        ['clean_last_name', 'clean_first_initial'], as_index=False)\n",
    "new_df = []\n",
    "for g, df in grouped_dups:\n",
    "    consol_df = df.apply(consolidate_row, axis=0)\n",
    "    consol_df['duplicate_dno'] = [df.dno.values]*df.shape[0]\n",
    "    new_df.append(consol_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df2 = pd.concat(new_df, axis=0)\n",
    "new_df3 = new_df2.drop_duplicates(['clean_first_name', 'clean_last_name', 'eod_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now, merge in manual corrections to add any more duplicate dnos\n",
    "# finally consolidate people merged into the same row\n",
    "df9 = pd.concat([df8.loc[pd.isnull(df8.duplicate_dno), :], new_df3], axis=0)\n",
    "\n",
    "\n",
    "# df7.loc[df7.duplicated(['clean_last_name', 'clean_medical_school'], keep=False), NAME_COLS+['clean_medical_school', 'eod_year', 'dno', 'duplicate_dno']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_remove = ['TERRECE', 'FRED', 'LAURENCE',\n",
    "             'CUONO', 'DEFRENZE', 'JEFFERY', 'FINKLEMAN', 'SHERRAD', 'ANSCHNETZ', 'MARC', 'JENSON', 'KASTI', \n",
    "            'ADELBERT', 'RITCHARD', 'MANSFORD', 'DEFRENZO', 'DROBIN', 'HAMES', 'KREUZ', 'JERROLD', 'MANEUSI',\n",
    "            'UNGARO']\n",
    "to_replace = ['TERRENCE', 'FREDERICK', 'LAWRENCE',\n",
    "              'CUOMO', 'DEFRONZO', 'JEFFREY', 'FINKELMAN', 'SHERRARD', 'ANSCHUETZ', 'MARCUS', 'JENSEN', 'KASTL',\n",
    "              'ALBERT', 'RITCHARD', 'MANIFORD', 'DEFRONZO', 'DROBIS', 'JAMES', 'KRUEZ', 'JERROD', 'MANCUSI',\n",
    "              'UNGARO']\n",
    "\n",
    "correct_name_mispellings_fnc = funcy.rpartial(correct_mispellings, to_remove, to_replace)\n",
    "\n",
    "df9.loc[:, 'clean_last_name'] = df9.clean_last_name.apply(correct_name_mispellings_fnc)\n",
    "df9.loc[:, 'clean_first_name'] = df9.clean_first_name.apply(correct_name_mispellings_fnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to go in and correct some of the name mispellings in both data sets\n",
    "# MUTATING FUNCTION\n",
    "def change_names(df, selection_type, selection_value, to_change_type, to_change_values):\n",
    "    for t, v in zip(to_change_type, to_change_values):\n",
    "        print t, v\n",
    "        df.loc[df[selection_type]==selection_value, t] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_first_name BRUCE\n",
      "clean_middle_name WILCOX\n",
      "clean_first_name MARC\n",
      "clean_middle_name I\n",
      "clean_first_name SIDNEY\n",
      "clean_middle_name CHARLES\n",
      "clean_medical_school WAKE_FOREST\n",
      "clean_first_name DONALD\n",
      "clean_middle_name MARTIN\n",
      "clean_first_name DONALD\n",
      "clean_middle_name MARTIN\n"
     ]
    }
   ],
   "source": [
    "change_names(\n",
    "    df9, 'clean_last_name', 'CHESEBRO', ['clean_first_name', 'clean_middle_name'], ['BRUCE', 'WILCOX'])\n",
    "change_names(df9, 'clean_last_name', 'GALANTER', ['clean_first_name', 'clean_middle_name'], ['MARC', 'I'])\n",
    "change_names(\n",
    "    df9, 'clean_last_name', 'BEAN', ['clean_first_name', 'clean_middle_name', 'clean_medical_school'], ['SIDNEY', 'CHARLES', 'WAKE_FOREST'])\n",
    "change_names(\n",
    "    df9, 'clean_last_name', 'EILER', ['clean_first_name', 'clean_middle_name'], ['DONALD', 'MARTIN'])\n",
    "change_names(\n",
    "    df9, 'clean_last_name', 'FALCHUK', ['clean_first_name', 'clean_middle_name'], ['DONALD', 'MARTIN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in missing eod year\n",
    "# manual corrections\n",
    "df9.loc[(df9.clean_last_name=='BEELS') & pd.isnull(df9.eod_year), 'eod_year'] = 1967\n",
    "df9.loc[(df9.clean_last_name=='KRAUSE') & pd.isnull(df9.eod_year), 'eod_year'] = 1965\n",
    "df9.loc[(df9.clean_last_name=='MELLMAN') & pd.isnull(df9.eod_year), 'eod_year'] = 1986\n",
    "# df9.loc[pd.isnull(df9.eod_year), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import manual eod year fixes\n",
    "manual_eod_fixes = pd.read_excel(os.path.join(CORRECTIONS_DIR, 'manual_eod_fixes.xlsx'))\n",
    "manual_eod_fixes.head()\n",
    "manual_eod_df = manual_eod_fixes[['clean_last_name'] + [c for c in manual_eod_fixes.columns if c.startswith('to_fix')]]\n",
    "\n",
    "missing_eod = pd.merge(\n",
    "    left=df9.loc[pd.isnull(df9.eod_year), :], right=manual_eod_df, on='clean_last_name', how='inner')\n",
    "\n",
    "for x in ['clean_medical_school', 'clean_first_name', 'clean_middle_name', 'eod_year']:\n",
    "    mask = ~pd.isnull(missing_eod['to_fix_{}'.format(x)])\n",
    "    missing_eod.loc[mask, x] = missing_eod.loc[mask, 'to_fix_{}'.format(x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_eod2 = missing_eod.drop(\n",
    "    (c for c in missing_eod.columns if c not in df9.columns), axis=1).drop_duplicates(['clean_last_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df10 = pd.concat([df9.loc[~pd.isnull(df9.eod_year), :], missing_eod2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_first_name</th>\n",
       "      <th>clean_middle_name</th>\n",
       "      <th>clean_last_name</th>\n",
       "      <th>eod_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DONALD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABELE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MARTIN</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>ABELOFF</td>\n",
       "      <td>1967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GARY</td>\n",
       "      <td>W</td>\n",
       "      <td>CAGE</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MING</td>\n",
       "      <td>TA</td>\n",
       "      <td>CHONG</td>\n",
       "      <td>1977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GERALD</td>\n",
       "      <td>A M</td>\n",
       "      <td>FINERMAN</td>\n",
       "      <td>1965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PETER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOLDMAN</td>\n",
       "      <td>1965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ERIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GROSSMAN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JOHN</td>\n",
       "      <td>B</td>\n",
       "      <td>HARLEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JAMES</td>\n",
       "      <td>MICHAEL</td>\n",
       "      <td>HERZOG</td>\n",
       "      <td>1973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>H</td>\n",
       "      <td>HOLSTEN</td>\n",
       "      <td>1967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JAY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KALAN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IMRAN</td>\n",
       "      <td>ULHAQ</td>\n",
       "      <td>KHAN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>LEVE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MALCOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARTIN</td>\n",
       "      <td>1964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JOHN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MENDELSOHN</td>\n",
       "      <td>1965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LESTER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERMUT</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ANTHONY</td>\n",
       "      <td>L</td>\n",
       "      <td>PICORE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IGNACIO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRATS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>STEPHEN</td>\n",
       "      <td>ISRAEL</td>\n",
       "      <td>RENNARD</td>\n",
       "      <td>1977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SCOTT</td>\n",
       "      <td>B</td>\n",
       "      <td>VANDE POL</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>STEPHEN</td>\n",
       "      <td>J</td>\n",
       "      <td>WINTERS</td>\n",
       "      <td>1974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JOSEPH</td>\n",
       "      <td>B</td>\n",
       "      <td>WARSHAW</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clean_first_name clean_middle_name clean_last_name  eod_year\n",
       "0            DONALD               NaN           ABELE       NaN\n",
       "1            MARTIN             DAVID         ABELOFF    1967.0\n",
       "2              GARY                 W            CAGE    1963.0\n",
       "3              MING                TA           CHONG    1977.0\n",
       "4            GERALD               A M        FINERMAN    1965.0\n",
       "5             PETER               NaN         GOLDMAN    1965.0\n",
       "6              ERIC               NaN        GROSSMAN       NaN\n",
       "7              JOHN                 B          HARLEY       NaN\n",
       "8             JAMES           MICHAEL          HERZOG    1973.0\n",
       "9            GEORGE                 H         HOLSTEN    1967.0\n",
       "10              JAY               NaN           KALAN       NaN\n",
       "11            IMRAN             ULHAQ            KHAN       NaN\n",
       "12                L             DAVID            LEVE       NaN\n",
       "13           MALCOM               NaN          MARTIN    1964.0\n",
       "14             JOHN               NaN      MENDELSOHN    1965.0\n",
       "15           LESTER               NaN          PERMUT    1985.0\n",
       "16          ANTHONY                 L          PICORE       NaN\n",
       "17          IGNACIO               NaN           PRATS       NaN\n",
       "18          STEPHEN            ISRAEL         RENNARD    1977.0\n",
       "19            SCOTT                 B       VANDE POL    1985.0\n",
       "20          STEPHEN                 J         WINTERS    1974.0\n",
       "21           JOSEPH                 B         WARSHAW    1963.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_eod2[NAME_COLS+['eod_year', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it seems that dno does refer to unique person, so drop dups based on that \n",
    "# save this unique to pick\n",
    "df10.to_pickle(os.path.join(PICKLE_DIR, 'unique_attendees.p'))\n",
    "\n",
    "# to csv\n",
    "df10.to_csv(os.path.join(ATT_DATA_DIR, 'unique_attendees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_eod_year = df10.loc[pd.isnull(df10.eod_year), :].sort_values('clean_last_name')\n",
    "\n",
    "missing_eod_year[NAME_COLS+['clean_medical_school', 'dno']].to_csv(os.path.join(ATT_DATA_DIR, 'missing_eod_year.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
