{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and compile NIH attendees\n",
    "# read in known applicant files, dedupe and try to merge with applicants file\n",
    "from collections import Counter\n",
    "import difflib\n",
    "import uuid\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import funcy\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_cleaning_functions import (trans_remov_punc, standardize_whitespace, remove_punc, remove_suffix_from_last_name,\n",
    "                                     clean_names, has_award, has_suffix, get_suffix, replace_last_name, \n",
    "                                     is_year_range, str_sim, clean_med_school, clean_std_college_name, long_form_date, \n",
    "                                    correct_mispellings)\n",
    "\n",
    "from dev import (\n",
    "    APP_DATA_DIR, SUM_STAT_DIR, RAW_ATT_DATA_DIR, ATT_DATA_DIR, CARD_DATA_DIR, CORRECTIONS_DIR, AWARDS_KEYWORDS, NAME_COLS, RAW_NAME_COLS, \n",
    "    RAW_CARD_ID, RAW_INDEX_IDS, PERSON_APPLICATION_ID, PERSON_ID, NIH_ID, FEMALE_FIRST_NAMES, FEMALE_MIDDLE_NAMES, \n",
    "    PICKLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concat_df['first_middle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_first_middle(raw_str):\n",
    "    if pd.isnull(raw_str):\n",
    "        return pd.Series({'firstname2': np.nan, 'middlename2': np.nan, 'suffix': np.nan})\n",
    "    # try to split on comma\n",
    "    lst = raw_str.split(', ')\n",
    "    lst2 = raw_str.split(' ')\n",
    "    if len(lst) == 1 and len(lst2)==1:\n",
    "        return pd.Series({'firstname2': lst[0], 'middlename2': np.nan, 'suffix': np.nan})\n",
    "    # now split on spaces\n",
    "    lst3 = map(remove_punc, lst2)\n",
    "    return pd.Series({'firstname2': lst3[0], 'middlename2': lst3[1], 'suffix': np.nan if len(lst3) < 3 else lst3[2]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "r1_file = '1964-1973 associates.XLS'\n",
    "r2_file = 'Associates alpha by institute.XLS'\n",
    "r3_file = 'Associates data.XLS'\n",
    "r4_file = 'NIMH Associates Complete.XLS'\n",
    "r5_file = 'NINDB Associates alpha by year.xls'\n",
    "filenames = [r1_file, r2_file, r3_file, r4_file, r5_file]\n",
    "\n",
    "file_df = map(lambda x: pd.read_excel(os.path.join(RAW_ATT_DATA_DIR, x)), filenames)\n",
    "\n",
    "file_4_columns = [\n",
    "    'dno', 'source', 'unknown', 'lastname', 'first_middle', 'institute', 'lab_brch', \n",
    "    'program', 'supervisor', 'eod_year', 'med_school', 'year_grad', 'intern_hos', 'intern_dte',\n",
    "       'res_hosp', 'residency', 'res_dtes'\n",
    "]\n",
    "\n",
    "file_df[2].rename(columns={'lname':'lastname', 'fname': 'first_middle'}, inplace=True)\n",
    "file_df[1].rename(columns={'lname':'lastname', 'fname': 'first_middle'}, inplace=True)\n",
    "\n",
    "file_df[4].columns = file_4_columns\n",
    "# for each files in the list, add a column to track source\n",
    "for name, f in zip(filenames, file_df):\n",
    "    f.loc[:, 'data_source'] = name\n",
    "concat_df = pd.concat(file_df)\n",
    "\n",
    "print sum(map(lambda x: x.shape[0], file_df)) == concat_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_a = concat_df.loc[:, 'first_middle'].apply(strip_first_middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we have all the associates, sep first middle into first and middle name, then sort and check \n",
    "# to see if we have any duplicates\n",
    "df2 = pd.concat([concat_df, df2_a], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_middle</th>\n",
       "      <th>firstname2</th>\n",
       "      <th>middlename2</th>\n",
       "      <th>suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas, Bruce</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>Bruce</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carmie</td>\n",
       "      <td>Carmie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael, C.</td>\n",
       "      <td>Michael</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daniel, N.</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roland</td>\n",
       "      <td>Roland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Oren, Wyatt</td>\n",
       "      <td>Oren</td>\n",
       "      <td>Wyatt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Howard, Laurence</td>\n",
       "      <td>Howard</td>\n",
       "      <td>Laurence</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>William, Zane</td>\n",
       "      <td>William</td>\n",
       "      <td>Zane</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Anthony, John</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Philip, Allen</td>\n",
       "      <td>Philip</td>\n",
       "      <td>Allen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>John, Shea</td>\n",
       "      <td>John</td>\n",
       "      <td>Shea</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Brian</td>\n",
       "      <td>Brian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gabriel, Martin</td>\n",
       "      <td>Gabriel</td>\n",
       "      <td>Martin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>William, Leon</td>\n",
       "      <td>William</td>\n",
       "      <td>Leon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Barry, Spencer</td>\n",
       "      <td>Barry</td>\n",
       "      <td>Spencer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thomas, Joseph III</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Albert, James</td>\n",
       "      <td>Albert</td>\n",
       "      <td>James</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lee, H</td>\n",
       "      <td>Lee</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>John, William Jr.</td>\n",
       "      <td>John</td>\n",
       "      <td>William</td>\n",
       "      <td>Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gyorgy, A</td>\n",
       "      <td>Gyorgy</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bruce, F</td>\n",
       "      <td>Bruce</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Richard E</td>\n",
       "      <td>Richard</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>John Kenneth</td>\n",
       "      <td>John</td>\n",
       "      <td>Kenneth</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>William Clayton</td>\n",
       "      <td>William</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ray, B.</td>\n",
       "      <td>Ray</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>William A. Jr.</td>\n",
       "      <td>William</td>\n",
       "      <td>A</td>\n",
       "      <td>Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Donald Martin</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Martin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ralph A.</td>\n",
       "      <td>Ralph</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Morris A</td>\n",
       "      <td>Morris</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Robert Saul</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Saul</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Thomas G.</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Roger, Alan</td>\n",
       "      <td>Roger</td>\n",
       "      <td>Alan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>John</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>George</td>\n",
       "      <td>George</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Allan, C.</td>\n",
       "      <td>Allan</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Sherman Charles</td>\n",
       "      <td>Sherman</td>\n",
       "      <td>Charles</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Michael, Lewis</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Lewis</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Chester, P. Jr.</td>\n",
       "      <td>Chester</td>\n",
       "      <td>P</td>\n",
       "      <td>Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>John</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Wayne, E.</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Peter</td>\n",
       "      <td>Peter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Nicholas, A.</td>\n",
       "      <td>Nicholas</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Frederic, Q.</td>\n",
       "      <td>Frederic</td>\n",
       "      <td>Q</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Gregory, O.</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Miles, M.</td>\n",
       "      <td>Miles</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Neal, Joseph</td>\n",
       "      <td>Neal</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>John, Nicholas</td>\n",
       "      <td>John</td>\n",
       "      <td>Nicholas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Elliot, Charles</td>\n",
       "      <td>Elliot</td>\n",
       "      <td>Charles</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Robert, Martin</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Martin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Fremont, P. Jr.</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>P</td>\n",
       "      <td>Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Kenneth, M.</td>\n",
       "      <td>Kenneth</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Arthur, H.</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Edward</td>\n",
       "      <td>Edward</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Vernon</td>\n",
       "      <td>Vernon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Philip.R.</td>\n",
       "      <td>Philip.R.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Albert, T. Jr.</td>\n",
       "      <td>Albert</td>\n",
       "      <td>T</td>\n",
       "      <td>Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Dean, Francis</td>\n",
       "      <td>Dean</td>\n",
       "      <td>Francis</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Andrew, Anthony</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Joseph, Harold</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>Harold</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8391 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           first_middle firstname2 middlename2 suffix\n",
       "0         Thomas, Bruce     Thomas       Bruce    NaN\n",
       "1                Carmie     Carmie         NaN    NaN\n",
       "2           Michael, C.    Michael           C    NaN\n",
       "3            Daniel, N.     Daniel           N    NaN\n",
       "4                Roland     Roland         NaN    NaN\n",
       "5           Oren, Wyatt       Oren       Wyatt    NaN\n",
       "6      Howard, Laurence     Howard    Laurence    NaN\n",
       "7         William, Zane    William        Zane    NaN\n",
       "8         Anthony, John    Anthony        John    NaN\n",
       "9         Philip, Allen     Philip       Allen    NaN\n",
       "10           John, Shea       John        Shea    NaN\n",
       "11                Brian      Brian         NaN    NaN\n",
       "12      Gabriel, Martin    Gabriel      Martin    NaN\n",
       "13        William, Leon    William        Leon    NaN\n",
       "14       Barry, Spencer      Barry     Spencer    NaN\n",
       "15   Thomas, Joseph III     Thomas      Joseph    III\n",
       "16        Albert, James     Albert       James    NaN\n",
       "17               Lee, H        Lee           H    NaN\n",
       "18    John, William Jr.       John     William     Jr\n",
       "19            Gyorgy, A     Gyorgy           A    NaN\n",
       "20             Bruce, F      Bruce           F    NaN\n",
       "21            Richard E    Richard           E    NaN\n",
       "22         John Kenneth       John     Kenneth    NaN\n",
       "23      William Clayton    William     Clayton    NaN\n",
       "24              Ray, B.        Ray           B    NaN\n",
       "25       William A. Jr.    William           A     Jr\n",
       "26        Donald Martin     Donald      Martin    NaN\n",
       "27             Ralph A.      Ralph           A    NaN\n",
       "28             Morris A     Morris           A    NaN\n",
       "29          Robert Saul     Robert        Saul    NaN\n",
       "..                  ...        ...         ...    ...\n",
       "147           Thomas G.     Thomas           G    NaN\n",
       "148         Roger, Alan      Roger        Alan    NaN\n",
       "149                John       John         NaN    NaN\n",
       "150              George     George         NaN    NaN\n",
       "151           Allan, C.      Allan           C    NaN\n",
       "152     Sherman Charles    Sherman     Charles    NaN\n",
       "153      Michael, Lewis    Michael       Lewis    NaN\n",
       "154     Chester, P. Jr.    Chester           P     Jr\n",
       "155                John       John         NaN    NaN\n",
       "156           Wayne, E.      Wayne           E    NaN\n",
       "157               Peter      Peter         NaN    NaN\n",
       "158              Thomas     Thomas         NaN    NaN\n",
       "159        Nicholas, A.   Nicholas           A    NaN\n",
       "160        Frederic, Q.   Frederic           Q    NaN\n",
       "161         Gregory, O.    Gregory           O    NaN\n",
       "162           Miles, M.      Miles           M    NaN\n",
       "163        Neal, Joseph       Neal      Joseph    NaN\n",
       "164      John, Nicholas       John    Nicholas    NaN\n",
       "165     Elliot, Charles     Elliot     Charles    NaN\n",
       "166      Robert, Martin     Robert      Martin    NaN\n",
       "167     Fremont, P. Jr.    Fremont           P     Jr\n",
       "168         Kenneth, M.    Kenneth           M    NaN\n",
       "169          Arthur, H.     Arthur           H    NaN\n",
       "170              Edward     Edward         NaN    NaN\n",
       "171              Vernon     Vernon         NaN    NaN\n",
       "172           Philip.R.  Philip.R.         NaN    NaN\n",
       "173      Albert, T. Jr.     Albert           T     Jr\n",
       "174       Dean, Francis       Dean     Francis    NaN\n",
       "175     Andrew, Anthony     Andrew     Anthony    NaN\n",
       "176      Joseph, Harold     Joseph      Harold    NaN\n",
       "\n",
       "[8391 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[~pd.isnull(df2.first_middle), ['first_middle', 'firstname2', 'middlename2', 'suffix']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# consolidate firstname columns\n",
    "df2.loc[~pd.isnull(df2.first_middle), 'firstname'] = df2.loc[~pd.isnull(df2.first_middle), 'firstname2']\n",
    "df2.loc[pd.isnull(df2.middlename), 'middlename'] = df2.loc[pd.isnull(df2.middlename), 'middlename2']\n",
    "\n",
    "df3 = df2.drop(['first_middle', 'firstname2', 'middlename2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/local/lib/python2.7/site-packages/pandas/core/indexing.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/local/lib/python2.7/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/attendees_data/unique_attendees.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4bc4123f2a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# it seems that dno does refer to unique person, so drop dups based on that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# save this unique to pick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf3_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATT_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_attendees.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \"\"\"\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_clipboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lraymond/MIT/Azoulay_2016/yellow_berets/yb/local/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mFile\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/attendees_data/unique_attendees.p'"
     ]
    }
   ],
   "source": [
    "# dropnow where both first and last name are missing\n",
    "df3 = df3.dropna(subset=['firstname', 'lastname'], how='all')\n",
    "\n",
    "# df3.dropna(subset=['firstname', 'lastname'], how='all').loc[:, ['firstname', 'lastname', 'dno', 'data_source']]\n",
    "df3.dropna(subset=['firstname', 'lastname'], how='all').loc[:, 'data_source'].unique()\n",
    "\n",
    "df3_sorted = df3.sort_values(by=['dno'])\n",
    "\n",
    "df3_unique = df3.drop_duplicates('dno')\n",
    "\n",
    "df3_unique.loc[:, 'clean_firstname'] = df3_unique['firstname'].apply(clean_names)\n",
    "df3_unique.loc[:, 'clean_middlename'] = df3_unique['middlename'].apply(clean_names)\n",
    "df3_unique.loc[:, 'clean_lastname'] = df3_unique['lastname'].apply(clean_names)\n",
    "\n",
    "df3_unique.sort_values(['clean_firstname', 'clean_middlename', 'clean_lastname'], inplace=True)\n",
    "\n",
    "df3_unique.loc[df3_unique.duplicated(['clean_firstname', 'clean_middlename', 'clean_lastname'], keep=False), :].to_csv(\n",
    "os.path.join(CARD_DATA_DIR, 'attendees_appearing_twice.csv'))\n",
    "\n",
    "df4 = df3_unique.drop_duplicates(['clean_firstname', 'clean_middlename', 'clean_lastname'], keep='first')\n",
    "\n",
    "# it seems that dno does refer to unique person, so drop dups based on that \n",
    "# save this unique to pick\n",
    "df3_unique.to_pickle(os.path.join(ATT_DATA_DIR, 'unique_attendees.p'))\n",
    "\n",
    "# to csv\n",
    "df3_unique.to_csv(os.path.join(ATT_DATA_DIR, 'unique_attendees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
