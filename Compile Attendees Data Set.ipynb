{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and compile NIH attendees\n",
    "# read in known applicant files, dedupe and try to merge with applicants file\n",
    "from collections import Counter\n",
    "import difflib\n",
    "import uuid\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import funcy\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "APP_DATA_DIR = '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/applicants_data'\n",
    "ATT_DATA_DIR = '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/attendees_data'\n",
    "RAW_ATT_DATA_DIR = '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/attendees_data/raw_NIH_data'\n",
    "CARD_DATA_DIR = '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/applicants_data/raw_card_data'\n",
    "SUM_STAT_DIR = '~/Dropbox (MIT)/yellowberets/lindsey/intermediate_data/summary stats/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r0_file = '1964-1973 associates.XLS'\n",
    "r2_file = 'Associates alpha by institute.XLS'\n",
    "r3_file = 'Associates data.XLS'\n",
    "r4_file = 'NIMH Associates Complete.XLS'\n",
    "r5_file = 'NINDB Associates alpha by year.xls'\n",
    "filenames = [r1_file, r2_file, r3_file, r4_file, r5_file]\n",
    "\n",
    "file_df = map(lambda x: pd.read_excel(os.path.join(RAW_ATT_DATA_DIR, x)), filenames)\n",
    "\n",
    "file_4_columns = [\n",
    "    'dno', 'source', 'unknown', 'lastname', 'first_middle', 'institute', 'lab_brch', \n",
    "    'program', 'supervisor', 'eod_year', 'med_school', 'year_grad', 'intern_hos', 'intern_dte',\n",
    "       'res_hosp', 'residency', 'res_dtes'\n",
    "]\n",
    "\n",
    "file_df[2].rename(columns={'lname':'lastname', 'fname': 'first_middle'}, inplace=True)\n",
    "file_df[1].rename(columns={'lname':'lastname', 'fname': 'first_middle'}, inplace=True)\n",
    "\n",
    "file_df[4].columns = file_4_columns\n",
    "# for each files in the list, add a column to track source\n",
    "for name, f in zip(filenames, file_df):\n",
    "    f.loc[:, 'data_source'] = name\n",
    "concat_df = pd.concat(file_df)\n",
    "\n",
    "print sum(map(lambda x: x.shape[0], file_df)) == concat_df.shape[0]\n",
    "\n",
    "# now we have all the associates, sep first middle into first and middle name, then sort and check \n",
    "# to see if we have any duplicates\n",
    "df2 = pd.concat([concat_df, concat_df.loc[:, 'first_middle'].apply(strip_first_middle)], axis=1)\n",
    "# consolidate firstname columns\n",
    "df2.loc[~pd.isnull(df2.first_middle), 'firstname'] = df2.loc[~pd.isnull(df2.first_middle), 'firstname2']\n",
    "df2.loc[pd.isnull(df2.middlename), 'middlename'] = df2.loc[pd.isnull(df2.middlename), 'middlename2']\n",
    "\n",
    "df3 = df2.drop(['first_middle', 'firstname2', 'middlename2'], axis=1)\n",
    "\n",
    "# dropnow where both first and last name are missing\n",
    "df3 = df3.dropna(subset=['firstname', 'lastname'], how='all')\n",
    "\n",
    "# df3.dropna(subset=['firstname', 'lastname'], how='all').loc[:, ['firstname', 'lastname', 'dno', 'data_source']]\n",
    "df3.dropna(subset=['firstname', 'lastname'], how='all').loc[:, 'data_source'].unique()\n",
    "\n",
    "df3_sorted = df3.sort_values(by=['dno'])\n",
    "\n",
    "df3_unique = df3.drop_duplicates('dno')\n",
    "\n",
    "df3_unique.loc[:, 'clean_firstname'] = df3_unique['firstname'].apply(clean_names)\n",
    "df3_unique.loc[:, 'clean_middlename'] = df3_unique['middlename'].apply(clean_names)\n",
    "df3_unique.loc[:, 'clean_lastname'] = df3_unique['lastname'].apply(clean_names)\n",
    "\n",
    "df3_unique.sort_values(['clean_firstname', 'clean_middlename', 'clean_lastname'], inplace=True)\n",
    "\n",
    "df3_unique.loc[df3_unique.duplicated(['clean_firstname', 'clean_middlename', 'clean_lastname'], keep=False), :].to_csv(\n",
    "os.path.join(CARD_DATA_DIR, 'attendees_appearing_twice.csv'))\n",
    "\n",
    "df4 = df3_unique.drop_duplicates(['clean_firstname', 'clean_middlename', 'clean_lastname'], keep='first')\n",
    "\n",
    "# it seems that dno does refer to unique person, so drop dups based on that \n",
    "# save this unique to pick\n",
    "df3_unique.to_pickle(os.path.join(ATT_DATA_DIR, 'unique_attendees.p'))\n",
    "\n",
    "# to csv\n",
    "df3_unique.to_csv(os.path.join(ATT_DATA_DIR, 'unique_attendees.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
